{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcAezKThdIHz0zaXBb7jgO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlesLclrc/TensorGAN/blob/main/test_function_gan_architecture_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1yY9bWShHLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c38e91-db03-435f-d9b0-010df4ec737b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#IMPORTS\n",
        "#basics\n",
        "from tensorflow.keras.layers import Input, Activation\n",
        "#conv\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Concatenate, MaxPooling2D, UpSampling2D  \n",
        "#others\n",
        "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Dropout\n",
        "from tensorflow.keras.layers import Layer, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.losses import Loss, BinaryCrossentropy\n",
        "from tensorflow import reduce_mean, ones_like,zeros_like\n",
        "from tensorflow.math import square, abs\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qE4d8skaolnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "photos = image_dataset_from_directory(directory='/content/drive/MyDrive/photo/photo3', labels = None, batch_size=477,shuffle=None)\n",
        "sketchs = image_dataset_from_directory(directory='/content/drive/MyDrive/sketch/sketch3', labels = None, batch_size=477,shuffle=None)"
      ],
      "metadata": {
        "id": "rNvLUmrbhM-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977bd44a-43dc-4711-92b8-503be6dcf240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 478 files belonging to 1 classes.\n",
            "Found 477 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = [next(iter(photos)).numpy(),next(iter(sketchs)).numpy()]\n",
        "y = next(iter(sketchs)).numpy()"
      ],
      "metadata": {
        "id": "V_PBgBQBmRQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ", strided_slice\n",
        "class Conv_bloc(Layer):\n",
        "    def __init__(self, \n",
        "                 filters,\n",
        "                 kernel_size = 3,\n",
        "                 batch_norm = True,\n",
        "                 max_pool=True,\n",
        "                 max_pool_size= 2,\n",
        "                 knl_init_stddev=0.2,\n",
        "                 lk_relu_alpha=0.2,\n",
        "                 drop_out_rate=None\n",
        "                 ):\n",
        "        super(Conv_bloc,self).__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.batch_norm = batch_norm\n",
        "        self.max_pool=max_pool\n",
        "        self.max_pool_size = max_pool_size\n",
        "        self.knl_init_stddev=knl_init_stddev\n",
        "        self.lk_relu_alpha=lk_relu_alpha\n",
        "        self.drop_out_rate = drop_out_rate\n",
        "\n",
        "        self.conv = Conv2D(filters = self.filters,\n",
        "                           kernel_size = self.kernel_size,\n",
        "                           padding = 'same',\n",
        "                           kernel_initializer = RandomNormal(stddev= self.knl_init_stddev))\n",
        "        self.skip = Conv2D(filters = self.filters,\n",
        "                           kernel_size = self.kernel_size,\n",
        "                           strides = self.max_pool_size if max_pool else 1,\n",
        "                           padding = 'same',\n",
        "                           kernel_initializer = RandomNormal(stddev= self.knl_init_stddev))\n",
        "\n",
        "        self.batch_normalization = BatchNormalization()\n",
        "        self.leakyrelu = LeakyReLU(alpha=self.lk_relu_alpha)\n",
        "        #self.max_pool2D = MaxPooling2D(pool_size=self.max_pool_size)\n",
        "        self.dropout = Dropout(self.drop_out_rate)\n",
        "        \n",
        "    def call(self, input):\n",
        "        conv = self.conv(input)\n",
        "        if self.batch_norm: conv = self.batch_normalization(conv,training=False)\n",
        "        conv = self.leakyrelu(conv)\n",
        "        skip = self.skip(conv)\n",
        "        if self.batch_norm: skip = self.batch_normalization(skip,training=False)\n",
        "        skip = self.leakyrelu(skip)\n",
        "        return conv, skip\n",
        "\n",
        "class Decod_bloc(Layer):\n",
        "    def __init__(self, \n",
        "                 filters,\n",
        "                 kernel_size = 3,\n",
        "                 batch_norm = True,\n",
        "                 max_pool=False,\n",
        "                 max_pool_size= 2,\n",
        "                 knl_init_stddev=0.2,\n",
        "                 drop_out_rate=None):\n",
        "        \n",
        "        super(Decod_bloc,self).__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.batch_norm = batch_norm\n",
        "        self.max_pool=max_pool\n",
        "        self.max_pool_size = max_pool_size\n",
        "        self.knl_init_stddev=knl_init_stddev\n",
        "        self.drop_out_rate = drop_out_rate\n",
        "        \n",
        "        #self.upsampling2D = UpSampling2D(size=self.max_pool_size)\n",
        "        \n",
        "        self.conv2DT = Conv2DTranspose(filters= self.filters, \n",
        "                        kernel_size= self.kernel_size, \n",
        "                        strides = self.max_pool_size if max_pool else 1,\n",
        "                        padding= 'same', \n",
        "                        kernel_initializer= RandomNormal(stddev= self.knl_init_stddev))\n",
        "        \n",
        "        self.batch_normalization = BatchNormalization()\n",
        "        self.dropout = Dropout(self.drop_out_rate)\n",
        "        self.concat = Concatenate()\n",
        "        self.conv1 = Conv2D(filters = self.filters,\n",
        "                  kernel_size = self.kernel_size,\n",
        "                  padding = 'same',\n",
        "                  kernel_initializer = RandomNormal(stddev= self.knl_init_stddev))\n",
        "        self.conv2 = Conv2D(filters = self.filters,\n",
        "                  kernel_size = self.kernel_size,\n",
        "                  padding = 'same',\n",
        "                  kernel_initializer = RandomNormal(stddev= self.knl_init_stddev))\n",
        "\n",
        "        \n",
        "    def call(self,input,skip_in):\n",
        "        #if self.max_pool: g = self.upsampling2D(input)\n",
        "        g = self.conv2DT(input)\n",
        "        if self.batch_norm: g = self.batch_normalization(g, training=False)\n",
        "        print(g,skip_in)\n",
        "        g = self.concat([g, skip_in])\n",
        "        # g = self.conv1(g)\n",
        "        # g = Activation('relu')(g)\n",
        "        # g = self.conv2(g)\n",
        "        # g = Activation('relu')(g)\n",
        "        if self.drop_out_rate: g = self.dropout(g, training=True)\n",
        "        return g"
      ],
      "metadata": {
        "id": "5qXqWbODhLfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(Model):\n",
        "    def __init__(self, \n",
        "                 kernel_size = 3,\n",
        "                 batch_norm = True,\n",
        "                 max_pool=True,\n",
        "                 max_pool_size= 2,\n",
        "                 knl_init_stddev=0.2,\n",
        "                 lk_relu_alpha=0.2,\n",
        "                 drop_out_rate=None,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "    \n",
        "        super(Generator,self).__init__()\n",
        "\n",
        "        #Conv_bloc & decoder kwargs\n",
        "        self.kernel_size = kernel_size\n",
        "        self.batch_norm = batch_norm\n",
        "        self.max_pool=max_pool\n",
        "        self.max_pool_size= max_pool_size\n",
        "        self.knl_init_stddev=knl_init_stddev\n",
        "        self.lk_relu_alpha=lk_relu_alpha\n",
        "        self.drop_out_rate=drop_out_rate\n",
        "        \n",
        "        kwargs = dict(kernel_size = self.kernel_size, \n",
        "                        batch_norm = self.batch_norm,\n",
        "                        max_pool = self.max_pool,\n",
        "                        max_pool_size = self.max_pool_size,\n",
        "                        knl_init_stddev = self.knl_init_stddev,\n",
        "                        lk_relu_alpha = self.lk_relu_alpha,\n",
        "                      drop_out_rate=self.drop_out_rate\n",
        "                      )\n",
        "\n",
        "        #Encoder\n",
        "\n",
        "        self.e1 = Conv_bloc(32,**kwargs)\n",
        "        self.e2 = Conv_bloc(64,**kwargs)\n",
        "        self.e3 = Conv_bloc(128,**kwargs)\n",
        "        self.e4 = Conv_bloc(256,**kwargs)\n",
        "        self.e5 = Conv_bloc(512,**kwargs)\n",
        "        self.bottleneck = Conv2D(512, \n",
        "                                 (4,4),  \n",
        "                                 strides = self.max_pool_size,\n",
        "                                 padding='same', \n",
        "                                 kernel_initializer=RandomNormal(stddev= self.knl_init_stddev))\n",
        "        \n",
        "        kwargs.pop('lk_relu_alpha')\n",
        "        #Decoder\n",
        "        kwargs['max_pool']=False\n",
        "        self.d1 = Decod_bloc(512,**kwargs)\n",
        "        kwargs['max_pool']=True\n",
        "        self.d2 = Decod_bloc(256,**kwargs)\n",
        "        self.d3 = Decod_bloc(128,**kwargs)\n",
        "        self.d4 = Decod_bloc(64,**kwargs)\n",
        "        self.d5 = Decod_bloc(32,**kwargs)\n",
        "        self.g = Conv2DTranspose(3,\n",
        "                        3,\n",
        "                        padding='same', \n",
        "                        kernel_initializer=RandomNormal(stddev = self.knl_init_stddev))\n",
        "        \n",
        "    def call(self,input):\n",
        "\n",
        "        conv, e1 = self.e1(input)\n",
        "        conv, e2 = self.e2(conv)\n",
        "        conv, e3 = self.e3(conv)\n",
        "        conv, e4 = self.e4(conv)\n",
        "        conv, e5 = self.e5(conv)\n",
        "\n",
        "        b = self.bottleneck(conv)\n",
        "        b = Activation('relu')(b)\n",
        "\n",
        "        print(b,e5)\n",
        "        conv = self.d1(b, e5) \n",
        "        print(conv)\n",
        "        conv = self.d2(conv, e4) \n",
        "        conv = self.d3(conv, e3) \n",
        "        conv = self.d4(conv, e2) \n",
        "        conv = self.d5(conv, e1) \n",
        "\n",
        "        g = self.g(conv)\n",
        "        out_image = Activation('sigmoid',name='generator')(g)\n",
        "\n",
        "        return out_image"
      ],
      "metadata": {
        "id": "NlXFxrhShQGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  class Discriminator(Model):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 kernel_size = (4,4),\n",
        "                 batch_norm = True,\n",
        "                 max_pool=True,\n",
        "                 max_pool_size= 1,\n",
        "                 knl_init_stddev=0.2,\n",
        "                 lk_relu_alpha=0.2,\n",
        "                 drop_out_rate=None,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        #Conv_bloc kwargs\n",
        "        self.kernel_size = kernel_size\n",
        "        self.batch_norm = batch_norm\n",
        "        self.max_pool = max_pool\n",
        "        self.max_pool_size = max_pool_size\n",
        "        self.knl_init_stddev = knl_init_stddev\n",
        "        self.lk_relu_alpha = lk_relu_alpha\n",
        "        self.drop_out_rate = drop_out_rate\n",
        "        \n",
        "        kwargs = dict(kernel_size = self.kernel_size, \n",
        "                      batch_norm = self.batch_norm,\n",
        "                      max_pool = self.max_pool,\n",
        "                      max_pool_size = self.max_pool_size,\n",
        "                      knl_init_stddev = self.knl_init_stddev,\n",
        "                      lk_relu_alpha = self.lk_relu_alpha,\n",
        "                      drop_out_rate = drop_out_rate)\n",
        "        \n",
        "        # discriminator layers\n",
        "        self.d1 = Conv_bloc(64,**kwargs)\n",
        "        self.d2 = Conv_bloc(128,**kwargs)\n",
        "        self.d3 = Conv_bloc(256,**kwargs)\n",
        "        self.d4 = Conv_bloc(512,**kwargs)\n",
        "        kwargs.pop('max_pool',None)\n",
        "        self.d5 = Conv_bloc(512, max_pool=False ,**kwargs)\n",
        "        # self.patch_out = Dense(1) \n",
        "        self.patch_out = Conv2D(1, \n",
        "                                (4,4), \n",
        "                                padding='same', \n",
        "                                kernel_initializer=RandomNormal(stddev=self.knl_init_stddev))\n",
        "        self.concat = Concatenate(axis=0)\n",
        "        self.concat_res = Concatenate(axis=-1)\n",
        "        \n",
        "    def call(self,pred_image,target_image):\n",
        "        d1 = self.d1(pred_image)\n",
        "        d2 = self.d2(d1)\n",
        "        d3 = self.d3(d2)\n",
        "        d4 = self.d4(d3)\n",
        "        d5 = self.d5(d4)\n",
        "        \n",
        "        patch_out = self.patch_out(d5)\n",
        "        discriminator_gen = Activation('sigmoid', name='discriminator')(patch_out)\n",
        "\n",
        "        d1 = self.d1(target_image)\n",
        "        d2 = self.d2(d1)\n",
        "        d3 = self.d3(d2)\n",
        "        d4 = self.d4(d3)\n",
        "        d5 = self.d5(d4)\n",
        "\n",
        "        patch_out = self.patch_out(d5)\n",
        "        discriminator_real = Activation('sigmoid', name='discriminator')(patch_out)\n",
        "\n",
        "        discriminators = self.concat_res([discriminator_gen, discriminator_real])\n",
        "\n",
        "        return discriminators"
      ],
      "metadata": {
        "id": "YJT4uerEhSir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input = Input(shape=(256,256,3))\n",
        "# d = Generator()(input)\n",
        "# output = Discriminator()(d,input)\n",
        "# Model(input,output).summary()"
      ],
      "metadata": {
        "id": "P3JvqeV6zyWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Gan_loss(Loss):\n",
        "    def __init__(self,coef=1):\n",
        "        super().__init__()\n",
        "        self.coef = coef\n",
        "        \n",
        "    def call(self, y_true, y_pred):\n",
        "        mae_loss = reduce_mean(abs(y_pred - y_true), axis=-1)\n",
        "        #sigmoid_cross_entropy = BinaryCrossentropy(from_logits=True)\n",
        "        #sigmoid_cross_entropy = sigmoid_cross_entropy(ones_like(y_pred), y_pred)\n",
        "        return self.coef*mae_loss# + sigmoid_cross_entropy\n",
        "\n",
        "class Disc_loss(Loss):        \n",
        "    def call(self, y_true, y_pred):\n",
        "        y_gen = y_pred[:,:,0]\n",
        "        y_real = y_pred[:,:,1]\n",
        "        sigmoid_cross_entropy = BinaryCrossentropy(from_logits=True)\n",
        "        gen_loss = sigmoid_cross_entropy(zeros_like(y_gen),y_gen)\n",
        "        real_loss = sigmoid_cross_entropy(ones_like(y_real),y_real)\n",
        "        total_loss = gen_loss + real_loss\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "0lTSbnykhUCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Pix2Pix(image_shape=(256,256,3)):\n",
        "    \n",
        "    discriminator1 = Discriminator()\n",
        "    discriminator2 = Discriminator()\n",
        "    generator = Generator()\n",
        "    \n",
        "    # for layer in discriminator1.layers:\n",
        "    #     if not isinstance(layer, BatchNormalization):\n",
        "    #         layer.trainable = False\n",
        "    \n",
        "    input_gen = Input(shape=image_shape)\n",
        "    input_disc = Input(shape=image_shape)\n",
        "    gen_out = generator(input_gen)\n",
        "    dis_out = discriminator1(gen_out,input_gen)\n",
        "    \n",
        "    model = Model(inputs=[input_gen, input_disc], outputs = [dis_out, gen_out])\n",
        "    \n",
        "    opt = Adam(learning_rate=0.0002, beta_1=0.75)\n",
        "    \n",
        "    model.compile(loss={dis_out.name.split('/')[0]:Disc_loss(), \n",
        "                     gen_out.name.split('/')[0]:Gan_loss(1e-5)}, \n",
        "               optimizer=opt, \n",
        "               loss_weights=[1,1])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "WxqCtpwVhVd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Pix2Pix()\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "15I3sakthW0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "oRjoXqtQRMvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.array_ops import batch_gather\n",
        "gen = Generator()\n",
        "gen.compile(loss ='mae',\n",
        "            optimizer = Adam(learning_rate=0.000002))\n",
        "gen.fit(X[0],y,epochs=100)"
      ],
      "metadata": {
        "id": "VXBahiH_nTk-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d05654c-d06a-4746-f519-380b941a374d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Tensor(\"generator_20/activation/Relu:0\", shape=(None, 128, 128, 512), dtype=float32) Tensor(\"generator_20/conv_bloc_104/leaky_re_lu_104/LeakyRelu_1:0\", shape=(None, 128, 128, 512), dtype=float32)\n",
            "Tensor(\"generator_20/decod_bloc_100/batch_normalization_205/FusedBatchNormV3:0\", shape=(None, 128, 128, 512), dtype=float32) Tensor(\"generator_20/conv_bloc_104/leaky_re_lu_104/LeakyRelu_1:0\", shape=(None, 128, 128, 512), dtype=float32)\n",
            "Tensor(\"generator_20/decod_bloc_100/concatenate_100/concat:0\", shape=(None, 128, 128, 1024), dtype=float32)\n",
            "Tensor(\"generator_20/decod_bloc_101/batch_normalization_206/FusedBatchNormV3:0\", shape=(None, 256, 256, 256), dtype=float32) Tensor(\"generator_20/conv_bloc_103/leaky_re_lu_103/LeakyRelu_1:0\", shape=(None, 128, 128, 256), dtype=float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-164-198a787eaa92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m gen.compile(loss ='mae',\n\u001b[1;32m      4\u001b[0m             optimizer = Adam(learning_rate=0.000002))\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file8fmg_hph.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileyljhcjqu.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, input, skip_in)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file8fmg_hph.py\", line 20, in tf__call\n        conv = ag__.converted_call(ag__.ld(self).d2, (ag__.ld(conv), ag__.ld(e4)), None, fscope)\n    File \"/tmp/__autograph_generated_fileyljhcjqu.py\", line 28, in tf__call\n        g = ag__.converted_call(ag__.ld(self).concat, ([ag__.ld(g), ag__.ld(skip_in)],), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"generator_20\" (type Generator).\n    \n    in user code:\n    \n        File \"<ipython-input-146-f85d5c3bb42a>\", line 74, in call  *\n            conv = self.d2(conv, e4)\n        File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileyljhcjqu.py\", line 28, in tf__call\n            g = ag__.converted_call(ag__.ld(self).concat, ([ag__.ld(g), ag__.ld(skip_in)],), None, fscope)\n    \n        ValueError: Exception encountered when calling layer \"decod_bloc_101\" (type Decod_bloc).\n        \n        in user code:\n        \n            File \"<ipython-input-156-aa59b3b8f87e>\", line 92, in call  *\n                g = self.concat([g, skip_in])\n            File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/usr/local/lib/python3.8/dist-packages/keras/layers/merging/concatenate.py\", line 123, in build\n                raise ValueError(err_msg)\n        \n            ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 256, 256, 256), (None, 128, 128, 256)]\n        \n        \n        Call arguments received by layer \"decod_bloc_101\" (type Decod_bloc):\n          • input=tf.Tensor(shape=(None, 128, 128, 1024), dtype=float32)\n          • skip_in=tf.Tensor(shape=(None, 128, 128, 256), dtype=float32)\n    \n    \n    Call arguments received by layer \"generator_20\" (type Generator):\n      • input=tf.Tensor(shape=(None, 256, 256, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=gen.predict(X[0][1].reshape(1,256,256,3))\n",
        "fig, (ax1,ax2) = plt.subplots(1,2)\n",
        "ax1.imshow((pred[0]+1)/2)\n",
        "ax2.imshow(y[1]/255)"
      ],
      "metadata": {
        "id": "4niJG-PvtdOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "gXE8_KSlOfpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[0].shape"
      ],
      "metadata": {
        "id": "m1ANMr0Yy0Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouxZ5JZatKdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X , y,epochs=1)"
      ],
      "metadata": {
        "id": "g1LABCOahZdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0][0].shape"
      ],
      "metadata": {
        "id": "kLThsCABhbU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toto = model.predict([X[0][0].reshape((1,256,256,3)),X[0][1].reshape((1,256,256,3))])"
      ],
      "metadata": {
        "id": "CDjd9E-ehcL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QbUO9evdhdR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toto[1].shape"
      ],
      "metadata": {
        "id": "DsBrffp9heUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(toto[1][0])"
      ],
      "metadata": {
        "id": "seOO2KNQhfiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "POK9I_FWk_Fs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}